---
title: "Druid学习"
subtitle: "Druid"
layout: post
author: "LiuL"
header-style: text
tags:
  - Druid
---

# 1. 安装启动访问

**Step 1: 下载**

下载地址：https://www.apache.org/dyn/closer.cgi?path=/druid/
下载最新版的Druid

**Step 2: 上传并解压**

上传到部署服务器下，在终端中运行以下命令来提取Druid

mkdir /var/lib/druid
tar -xzf apache-druid-0.20.0-bin.tar.gz -C /var/lib/druid

**Step 3:设置环境变量**

sudo echo 'export DRUID_HOME=/var/lib/druid' >> /etc/profile
source /etc/profile

**Step 4：启动**
cd $DRUID_HOME
./bin/start-micro-quickstart

**Step 4：访问Druid控制台**

可以访问http://ip:8888/unified-console.html来Druid控制台，控制台由Druid Router进程启动。

# 2. 从本地导入数据

## 2.1. 使用Druid控制台-Data Loader来加载数据

**Step 1：进入Load data**

点击控制台中的 Load data

![20201209161135](https://liulv.work/images/img/20201209161135.png)


**Step 2：选择本地磁盘**

选择 Local disk 然后点击 Connect data

![20201209161340](https://liulv.work/images/img/20201209161340.png)

**Step 3: 设置Connect**

在 Base directory 中输入 quickstart/tutorial/, 在 File filter 中输入 wikiticker-2015-09-12-sampled.json.gz。 Base directory 和 File filter 分开是因为可能需要同时从多个文件中摄取数据。然后点击apply确保您看到的数据是正确的，再点击Next: Parse data

![20201209161603](https://liulv.work/images/img/20201209161603.png)

![20201209161703](https://liulv.work/images/img/20201209161703.png)

**Step 4: 设置Parse data**

输入格式设置json, 点击下一步

![20201209161858](https://liulv.work/images/img/20201209161858.png)

**Step 5: 设置Parse time**

设置的分区列，这是默认设置time列即可，点击下一步

![20201209162047](https://liulv.work/images/img/20201209162047.png)

**Step 6: 设置转换**

可点击对应列，进行调整转换，这里不需要任何转换，可直接下一步

![20201209162311](https://liulv.work/images/img/20201209162311.png)

**Step 7: 设置过滤**

可点击对应列进行过滤设置，设置过滤值，这里不需要任何过滤，可直接下一步

![20201209162435](https://liulv.work/images/img/20201209162435.png)

**Step 7: 调整schema-列名和类型等**

可选择对应列进行修改，这里全部使用json默认的，可直接下一步

![20201209162654](https://liulv.work/images/img/20201209162654.png)


**Step 8: 设置分区**

分区类型可设置统一的和任意的，这里设置统一的，并且按天进行分区

![20201209162852](https://liulv.work/images/img/20201209162852.png)


**Step 9: 调优设置**

这里不进行任何优化，直接下一步提交

![20201209163005](https://liulv.work/images/img/20201209163005.png)

**Step 10: 提交设置**

修改数据源名称

![20201209163119](https://liulv.work/images/img/20201209163119.png)

**Step 11: 修改json规范**

这里暂时不需要修改，直接提交即可

![20201209163217](https://liulv.work/images/img/20201209163217.png)


**Step 12: 提交完成会进入到任务列表**

![20201209163811](https://liulv.work/images/img/20201209163811.png)

可点击详情按钮查看运行情况

![20201209163914](https://liulv.work/images/img/20201209163914.png)

![20201209164006](https://liulv.work/images/img/20201209164006.png)

**Step 13: 查看数据源并查询数据**

点击Datasource是否已经新增wikiticker-test

![20201209164137](https://liulv.work/images/img/20201209164137.png)

点击Query进行数据查询

![20201209164235](https://liulv.work/images/img/20201209164235.png)

## 2.2. 使用命令行方式加载数据

为了方便，在Druid的软件包中提供了一个批摄取的帮助脚本 bin/post-index-task

该脚本会将数据摄取任务发布到Druid Overlord并轮询Druid，直到可以查询数据为止。

在Druid根目录运行以下命令：

```powershell
bin/post-index-task --file quickstart/tutorial/wikipedia-index.json --url http://localhost:8081
```



可以看到以下的输出：

```log
Beginning indexing data for wikipedia
Task started: index_wikipedia_2018-07-27T06:37:44.323Z
Task log:     http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/log
Task status:  http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/status
Task index_wikipedia_2018-07-27T06:37:44.323Z still running...
Task index_wikipedia_2018-07-27T06:37:44.323Z still running...
Task finished with status: SUCCESS
Completed indexing data for wikipedia. Now loading indexed data onto the cluster...
wikipedia loading complete! You may now query your data
```

提交任务规范后，您可以按照上述相同的规范等待数据加载然后查询

## 2.3. 使用HTTP请求加载数据

我们简短地讨论一下如何在不使用脚本的情况下提交数据摄取任务，您将不需要运行这些命令。

要提交任务，可以在一个新的终端中通过以下方式提交任务到Druid：

```powershell
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8081/druid/indexer/v1/task
```

当任务提交成功后会打印出来任务的ID

     {"task":"index_wikipedia_2018-06-09T21:30:32.802Z"}

您可以如上所述从控制台监视此任务的状态

# 3. 查询数据

本教程将以Druid SQL和Druid的原生查询格式的示例演示如何在Apache Druid中查询数据。

本教程假定您已经完成了摄取教程之一，因为我们将查询Wikipedia编辑样例数据。

Druid支持SQL查询。

该查询检索了2015年9月12日被编辑最多的10个维基百科页面

```sql
SELECT page, COUNT(*) AS Edits
FROM wikipedia
WHERE TIMESTAMP '2015-09-12 00:00:00' <= "__time" AND "__time" < TIMESTAMP '2015-09-13 00:00:00'
GROUP BY page
ORDER BY Edits DESC
LIMIT 10
```

让我们来看几种不同的查询方法

## 3.1. 通过控制台Query查询

![20201209165417](https://liulv.work/images/img/20201209165417.png)

## 3.2. 通过dsql查询SQL

为方便起见，Druid软件包中包括了一个SQL命令行客户端，位于Druid根目录中的 bin/dsql

运行 bin/dsql, 可以看到如下：

Welcome to dsql, the command-line client for Druid SQL.
Type "\h" for help.
dsql>

![20201209165626](https://liulv.work/images/img/20201209165626.png)

## 3.3. 通过HTTP查询SQL

SQL查询作为JSON通过HTTP提交

教程包括一个示例文件, 该文件quickstart/tutorial/wikipedia-top-pages-sql.json包含上面显示的SQL查询, 我们将该查询提交给Druid Broker。

```http
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8888/druid/v2/sql
```

## 3.4. 更多Druid SQL示例

### 3.4.1. 时间查询

```sql
SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'
GROUP BY 1
```

![20201209165956](https://liulv.work/images/img/20201209165956.png)

### 3.4.2. 聚合查询

```sql
SELECT channel, page, SUM(added)
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'
GROUP BY channel, page
ORDER BY SUM(added) DESC
```

![20201209170146](https://liulv.work/images/img/20201209170146.png)

### 3.4.3. 查询原始数据

```sql
SELECT user, page
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00'
LIMIT 5
```

![20201209172941](https://liulv.work/images/img/20201209172941.png)

### 3.4.4. 查看执行计划

点击运行旁边... Explain SQL query

![20201209173045](https://liulv.work/images/img/20201209173045.png)

![20201209173114](https://liulv.work/images/img/20201209173114.png)

如果您以其他方式查询，则可以通过在Druid SQL查询之前添加 EXPLAIN PLAN FOR 来获得查询计划。

使用上边的一个示例：

EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;

```sql
dsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;
┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    │
├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ DruidQueryRel(query=[{"queryType":"topN","dataSource":{"type":"table","name":"wikipedia"},"virtualColumns":[],"dimension":{"type":"default","dimension":"page","outputName":"d0","outputType":"STRING"},"metric":{"type":"numeric","metric":"a0"},"threshold":10,"intervals":{"type":"intervals","intervals":["2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z"]},"filter":null,"granularity":{"type":"all"},"aggregations":[{"type":"count","name":"a0"}],"postAggregations":[],"context":{},"descending":false}], signature=[{d0:STRING, a0:LONG}]) │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
Retrieved 1 row in 0.03s.
```


# 4. 特别注意1

Druid Hadoop Docker安装之前需要关闭SELinux


``` shell
getenforce
```

返回
Enforcing

执行关闭SELinux

``` shell
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```

检查SELinux

``` shell
getenforce
```


# 5. Roll-up聚合操作

Apache Druid可以通过roll-up在数据摄取阶段对原始数据进行汇总。 Roll-up是对选定列集的一级聚合操作，它可以减小存储数据的大小。

本教程中将讨论在一个示例数据集上进行roll-up的结果。

本教程我们假设您已经按照单服务器部署中描述下载了Druid，并运行在本地机器上。

完成加载本地文件和数据查询两部分内容也是非常有帮助的。

## 5.1. 数据准备

使用一个网络流事件数据的小样本，表示在特定时间内从源到目标IP地址的流量的数据包和字节计数。

```json
{"timestamp":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":20,"bytes":9024}
{"timestamp":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":255,"bytes":21133}
{"timestamp":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":11,"bytes":5780}
{"timestamp":"2018-01-01T01:02:14Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":38,"bytes":6289}
{"timestamp":"2018-01-01T01:02:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":377,"bytes":359971}
{"timestamp":"2018-01-01T01:03:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":49,"bytes":10204}
{"timestamp":"2018-01-02T21:33:14Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":38,"bytes":6289}
{"timestamp":"2018-01-02T21:33:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":123,"bytes":93999}
{"timestamp":"2018-01-02T21:35:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":12,"bytes":2818}
```

示例数据在Druid安装目录`quickstart/tutorial/rollup-data.json`下，我们将使用此接入数据规范来接入数据。

```json
{
  "type" : "index_parallel",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "dimensionsSpec" : {
        "dimensions" : [
          "srcIP",
          "dstIP"
        ]
      },
      "timestampSpec": {
        "column": "timestamp",
        "format": "iso"
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index_parallel",
      "inputSource" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "inputFormat" : {
        "type" : "json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}
```

通过在 granularitySpec 选项中设置 rollup : true 来启用Roll-up

注意，我们将srcIP和dstIP定义为维度，将packets和bytes列定义为了longSum类型的指标，并将 queryGranularity 配置定义为 minute。

加载这些数据后，我们将看到如何使用这些定义。

## 5.2. 加载数据

在Druid的根目录下运行以下命令：

```powershell
bin/post-index-task --file quickstart/tutorial/rollup-index.json --url http://localhost:8081
```

脚本运行完成以后，我们将查询数据。

![20201210145621](https://liulv.work/images/img/20201210145621.png)

## 5.3. 查询数据

现在运行 bin/dsql 然后执行查询 select * from "rollup-tutorial"; 来查看已经被摄入的数据

```powershell
[root@localhost apache-druid-0.20.0]# bin/dsql
Welcome to dsql, the command-line client for Druid SQL.
Connected to [http://localhost:8082/].

Type "\h" for help.
dsql> select * from "rollup-tutorial";
┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐
│ __time                   │ bytes  │ count │ dstIP   │ packets │ srcIP   │
├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤
│ 2018-01-01T01:01:00.000Z │  35937 │     3 │ 2.2.2.2 │     286 │ 1.1.1.1 │
│ 2018-01-01T01:02:00.000Z │ 366260 │     2 │ 2.2.2.2 │     415 │ 1.1.1.1 │
│ 2018-01-01T01:03:00.000Z │  10204 │     1 │ 2.2.2.2 │      49 │ 1.1.1.1 │
│ 2018-01-02T21:33:00.000Z │ 100288 │     2 │ 8.8.8.8 │     161 │ 7.7.7.7 │
│ 2018-01-02T21:35:00.000Z │   2818 │     1 │ 8.8.8.8 │      12 │ 7.7.7.7 │
└──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘
Retrieved 5 rows in 0.18s.
```

我们来看发生在 2018-01-01T01:01 的三条原始数据：

```json
{"timestamp":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":20,"bytes":9024}
{"timestamp":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":255,"bytes":21133}
{"timestamp":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":11,"bytes":5780}
```

这三条数据已经被roll up为以下一行数据：

```powershell
┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐
│ __time                   │ bytes  │ count │ dstIP   │ packets │ srcIP   │
├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤
│ 2018-01-01T01:01:00.000Z │  35937 │     3 │ 2.2.2.2 │     286 │ 1.1.1.1 │
└──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘
```

这输入的数据行已经被按照时间列和维度列 {timestamp, srcIP, dstIP} 在指标列 {packages, bytes} 上做求和聚合.

在进行分组之前，原始输入数据的时间戳按分钟进行标记/布局，这是由于摄取规范中的 "queryGranularity"："minute" 设置造成的。 同样，2018-01-01T01:02 期间发生的这两起事件也已经汇总。


# 6. 配置数据保留规则

假设我们想删除2015年9月12日前12小时的数据，保留2015年9月12日后12小时的数据。

进入到Datasources视图，点击编辑按钮-然后再点击Edit retention rules

![20201210150714](https://liulv.work/images/img/20201210150714.png)

一个规则配置窗口出现了：

![20201210150821](https://liulv.work/images/img/20201210150821.png)

现在点击 + New rule 按钮两次

在上边的规则框中，选择 Load 和 by Interval 然后输入在 by Interval 旁边的输入框中输入 2015-09-12T12:00:00.000Z/2015-09-13T00:00:00.000Z, 副本可以选择保持2，在 _default_tier 中

在下边的规则框中，选择 Drop 和 forever

规则看上去是这样的：

![20201210151143](https://liulv.work/images/img/20201210151143.png)


现在点击 Next, 规则配置过程将要求提供用户名和注释，以便进行更改日志记录。您可以同时输入教程。

现在点击 Save, 可以在Datasources视图中看到新的规则

![20201210151757](https://liulv.work/images/img/20201210151757.png)

给集群几分钟时间应用规则更改，然后转到Druid控制台中的segments视图。2015年9月12日前12小时的段文件现已消失

![20201210151931](https://liulv.work/images/img/20201210151931.png)

生成的保留规则链如下:

loadByInterval 2015-09-12T12/2015-09-13 (12 hours)
dropForever
loadForever (默认规则)
规则链是自上而下计算的，默认规则链始终添加在底部

我们刚刚创建的教程规则链在指定的12小时间隔内加载数据

如果数据不在12小时的间隔内，则规则链下一步将计算 dropForever，这将删除任何数据

dropForever 终止了规则链，有效地覆盖了默认的 loadForever 规则，在这个规则链中永远不会到达该规则

注意，在本教程中，我们定义了一个特定间隔的加载规则

相反，如果希望根据数据的生命周期保留数据（例如，保留从过去3个月到现在3个月的数据），则应定义一个周期性加载规则


# 7. 数据更新

本教程演示如何更新现有数据，同时展示覆盖(Overwrite)和追加(append)的两个方式。

## 7.1. 数据覆盖Overwrite

### 7.1.1. 加载初始数据

本节教程使用的任务摄取规范位于 quickstart/tutorial/updates-init-index.json, 本规范从 quickstart/tutorial/updates-data.json 输入文件创建一个名称为 updates-tutorial 的数据源

提交任务：

```powershell
bin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081
```

我们有三个包含"动物"维度和"数字"指标的初始行：

```sql
select * from "updates-tutorial";
```

![20201210153051](https://liulv.work/images/img/20201210153051.png)

### 7.1.2. 将旧数据与新数据合并并覆盖

现在我们尝试在 updates-tutorial 数据源追加一些新的数据，我们将从 quickstart/tutorial/updates-data3.json 增加新的数据

quickstart/tutorial/updates-append-index.json 任务规范配置为从现有的 updates-tutorial 数据源和 quickstart/tutorial/updates-data3.json 文件读取数据，该任务将组合来自两个输入源的数据，然后用新的组合数据覆盖原始数据。

提交任务：

```sql
bin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081
```

查询合并后的数据

```sql
select * from "updates-tutorial";
```

当Druid完成从这个覆盖任务加载新段时，新行将被添加到数据源中。请注意，“Lion”行发生了roll up：

```log
[root@localhost apache-druid-0.20.0]# bin/dsql

Welcome to dsql, the command-line client for Druid SQL.
Connected to [http://localhost:8082/].

Type "\h" for help.
dsql> 
dsql> 
dsql> select * from "updates-tutorial";
┌──────────────────────────┬──────────┬───────┬────────┐
│ __time                   │ animal   │ count │ number │
├──────────────────────────┼──────────┼───────┼────────┤
│ 2018-01-01T01:01:00.000Z │ lion     │     1 │    300 │
│ 2018-01-01T01:01:00.000Z │ tiger    │     1 │    100 │
│ 2018-01-01T03:01:00.000Z │ aardvark │     1 │     42 │
│ 2018-01-01T03:01:00.000Z │ giraffe  │     1 │  14124 │
│ 2018-01-01T05:01:00.000Z │ mongoose │     1 │    737 │
│ 2018-01-01T06:01:00.000Z │ snake    │     1 │   1234 │
│ 2018-01-01T07:01:00.000Z │ octopus  │     1 │    115 │
└──────────────────────────┴──────────┴───────┴────────┘
Retrieved 7 rows in 0.13s.
```

### 7.1.3. 追加数据

现在尝试另一种追加数据的方式

quickstart/tutorial/updates-append-index2.json 任务规范从 quickstart/tutorial/updates-data4.json 文件读取数据，然后追加到 updates-tutorial 数据源。注意到在规范中 appendToExisting 设置为 true

提交任务：

bin/post-index-task --file quickstart/tutorial/updates-append-index2.json --url http://localhost:8081

加载新数据后，我们可以看到"octopus"后面额外的两行。请注意，编号为222的新"bear"行尚未与现有的bear-111行合并，因为新数据保存在单独的段中。

```sql
select * from "updates-tutorial";
```

# 8. 合并段文件

