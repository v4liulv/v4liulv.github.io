---
title: "Druid学习"
subtitle: "Druid"
layout: post
author: "LiuL"
header-style: text
tags:
  - Druid
---

# 1. 安装启动访问

**Step 1: 下载**

下载地址：https://www.apache.org/dyn/closer.cgi?path=/druid/
下载最新版的Druid

**Step 2: 上传并解压**

上传到部署服务器下，在终端中运行以下命令来提取Druid

mkdir /var/lib/druid
tar -xzf apache-druid-0.20.0-bin.tar.gz -C /var/lib/druid

**Step 3:设置环境变量**

sudo echo 'export DRUID_HOME=/var/lib/druid' >> /etc/profile
source /etc/profile

**Step 4：启动**
cd $DRUID_HOME
./bin/start-micro-quickstart

**Step 4：访问Druid控制台**

可以访问http://ip:8888/unified-console.html来Druid控制台，控制台由Druid Router进程启动。

# 2. 从本地导入数据

## 2.1. 使用Druid控制台-Data Loader来加载数据

**Step 1：进入Load data**

点击控制台中的 Load data

![20201209161135](https://liulv.work/images/img/20201209161135.png)


**Step 2：选择本地磁盘**

选择 Local disk 然后点击 Connect data

![20201209161340](https://liulv.work/images/img/20201209161340.png)

**Step 3: 设置Connect**

在 Base directory 中输入 quickstart/tutorial/, 在 File filter 中输入 wikiticker-2015-09-12-sampled.json.gz。 Base directory 和 File filter 分开是因为可能需要同时从多个文件中摄取数据。然后点击apply确保您看到的数据是正确的，再点击Next: Parse data

![20201209161603](https://liulv.work/images/img/20201209161603.png)

![20201209161703](https://liulv.work/images/img/20201209161703.png)

**Step 4: 设置Parse data**

输入格式设置json, 点击下一步

![20201209161858](https://liulv.work/images/img/20201209161858.png)

**Step 5: 设置Parse time**

设置的分区列，这是默认设置time列即可，点击下一步

![20201209162047](https://liulv.work/images/img/20201209162047.png)

**Step 6: 设置转换**

可点击对应列，进行调整转换，这里不需要任何转换，可直接下一步

![20201209162311](https://liulv.work/images/img/20201209162311.png)

**Step 7: 设置过滤**

可点击对应列进行过滤设置，设置过滤值，这里不需要任何过滤，可直接下一步

![20201209162435](https://liulv.work/images/img/20201209162435.png)

**Step 7: 调整schema-列名和类型等**

可选择对应列进行修改，这里全部使用json默认的，可直接下一步

![20201209162654](https://liulv.work/images/img/20201209162654.png)


**Step 8: 设置分区**

分区类型可设置统一的和任意的，这里设置统一的，并且按天进行分区

![20201209162852](https://liulv.work/images/img/20201209162852.png)


**Step 9: 调优设置**

这里不进行任何优化，直接下一步提交

![20201209163005](https://liulv.work/images/img/20201209163005.png)

**Step 10: 提交设置**

修改数据源名称

![20201209163119](https://liulv.work/images/img/20201209163119.png)

**Step 11: 修改json规范**

这里暂时不需要修改，直接提交即可

![20201209163217](https://liulv.work/images/img/20201209163217.png)


**Step 12: 提交完成会进入到任务列表**

![20201209163811](https://liulv.work/images/img/20201209163811.png)

可点击详情按钮查看运行情况

![20201209163914](https://liulv.work/images/img/20201209163914.png)

![20201209164006](https://liulv.work/images/img/20201209164006.png)

**Step 13: 查看数据源并查询数据**

点击Datasource是否已经新增wikiticker-test

![20201209164137](https://liulv.work/images/img/20201209164137.png)

点击Query进行数据查询

![20201209164235](https://liulv.work/images/img/20201209164235.png)

## 2.2. 使用命令行方式加载数据

为了方便，在Druid的软件包中提供了一个批摄取的帮助脚本 bin/post-index-task

该脚本会将数据摄取任务发布到Druid Overlord并轮询Druid，直到可以查询数据为止。

在Druid根目录运行以下命令：

```powershell
bin/post-index-task --file quickstart/tutorial/wikipedia-index.json --url http://localhost:8081
```



可以看到以下的输出：

```log
Beginning indexing data for wikipedia
Task started: index_wikipedia_2018-07-27T06:37:44.323Z
Task log:     http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/log
Task status:  http://localhost:8081/druid/indexer/v1/task/index_wikipedia_2018-07-27T06:37:44.323Z/status
Task index_wikipedia_2018-07-27T06:37:44.323Z still running...
Task index_wikipedia_2018-07-27T06:37:44.323Z still running...
Task finished with status: SUCCESS
Completed indexing data for wikipedia. Now loading indexed data onto the cluster...
wikipedia loading complete! You may now query your data
```

提交任务规范后，您可以按照上述相同的规范等待数据加载然后查询

## 2.3. 使用HTTP请求加载数据

我们简短地讨论一下如何在不使用脚本的情况下提交数据摄取任务，您将不需要运行这些命令。

要提交任务，可以在一个新的终端中通过以下方式提交任务到Druid：

```powershell
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-index.json http://localhost:8081/druid/indexer/v1/task
```

当任务提交成功后会打印出来任务的ID

     {"task":"index_wikipedia_2018-06-09T21:30:32.802Z"}

您可以如上所述从控制台监视此任务的状态

# 3. 从Hadoop加载数据

本章节介绍如何使用远程Hadoop集群将数据文件加载到Apache Druid中。

## 3.1. 前提

已经在同一可以互通网络安装了Druid 和 Hadoop, 并且都已经启动

## 拷贝数据到shared目录

```powershell
cp $DRUID_HOME/quickstart/tutorial/wikiticker-2015-09-12-sampled.json.gz /tmp/shared/wikiticker-2015-09-12-sampled.json.gz
```

## 设置Hadoop目录

在Hadoop容器shell中，运行以下命令来设置本次教程需要的HDFS目录，同时拷贝输入数据到HDFS上

hdfs dfs -mkdir /druid
hdfs dfs -mkdir /druid/segments
hdfs dfs -mkdir /quickstart
hdfs dfs -chmod 777 /druid
hdfs dfs -chmod 777 /druid/segments
hdfs dfs -chmod 777 /quickstart
hdfs dfs -chmod -R 777 /tmp
hdfs dfs -chmod -R 777 /user
hdfs dfs -put /shared/wikiticker-2015-09-12-sampled.json.gz /quickstart/wikiticker-2015-09-12-sampled.json.gz




# 4. 查询数据

本教程将以Druid SQL和Druid的原生查询格式的示例演示如何在Apache Druid中查询数据。

本教程假定您已经完成了摄取教程之一，因为我们将查询Wikipedia编辑样例数据。

Druid支持SQL查询。

该查询检索了2015年9月12日被编辑最多的10个维基百科页面

```sql
SELECT page, COUNT(*) AS Edits
FROM wikipedia
WHERE TIMESTAMP '2015-09-12 00:00:00' <= "__time" AND "__time" < TIMESTAMP '2015-09-13 00:00:00'
GROUP BY page
ORDER BY Edits DESC
LIMIT 10
```

让我们来看几种不同的查询方法

## 4.1. 通过控制台Query查询

![20201209165417](https://liulv.work/images/img/20201209165417.png)

## 4.2. 通过dsql查询SQL

为方便起见，Druid软件包中包括了一个SQL命令行客户端，位于Druid根目录中的 bin/dsql

运行 bin/dsql, 可以看到如下：

Welcome to dsql, the command-line client for Druid SQL.
Type "\h" for help.
dsql>

![20201209165626](https://liulv.work/images/img/20201209165626.png)

## 4.3. 通过HTTP查询SQL

SQL查询作为JSON通过HTTP提交

教程包括一个示例文件, 该文件quickstart/tutorial/wikipedia-top-pages-sql.json包含上面显示的SQL查询, 我们将该查询提交给Druid Broker。

```http
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/wikipedia-top-pages-sql.json http://localhost:8888/druid/v2/sql
```

## 4.4. 更多Druid SQL示例

### 4.4.1. 时间查询

```sql
SELECT FLOOR(__time to HOUR) AS HourTime, SUM(deleted) AS LinesDeleted
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'
GROUP BY 1
```

![20201209165956](https://liulv.work/images/img/20201209165956.png)

### 4.4.2. 聚合查询

```sql
SELECT channel, page, SUM(added)
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00'
GROUP BY channel, page
ORDER BY SUM(added) DESC
```

![20201209170146](https://liulv.work/images/img/20201209170146.png)

### 4.4.3. 查询原始数据

```sql
SELECT user, page
FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 02:00:00' AND TIMESTAMP '2015-09-12 03:00:00'
LIMIT 5
```

![20201209172941](https://liulv.work/images/img/20201209172941.png)

### 4.4.4. 查看执行计划

点击运行旁边... Explain SQL query

![20201209173045](https://liulv.work/images/img/20201209173045.png)

![20201209173114](https://liulv.work/images/img/20201209173114.png)

如果您以其他方式查询，则可以通过在Druid SQL查询之前添加 EXPLAIN PLAN FOR 来获得查询计划。

使用上边的一个示例：

EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;

```sql
dsql> EXPLAIN PLAN FOR SELECT page, COUNT(*) AS Edits FROM wikipedia WHERE "__time" BETWEEN TIMESTAMP '2015-09-12 00:00:00' AND TIMESTAMP '2015-09-13 00:00:00' GROUP BY page ORDER BY Edits DESC LIMIT 10;
┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ PLAN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    │
├─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤
│ DruidQueryRel(query=[{"queryType":"topN","dataSource":{"type":"table","name":"wikipedia"},"virtualColumns":[],"dimension":{"type":"default","dimension":"page","outputName":"d0","outputType":"STRING"},"metric":{"type":"numeric","metric":"a0"},"threshold":10,"intervals":{"type":"intervals","intervals":["2015-09-12T00:00:00.000Z/2015-09-13T00:00:00.001Z"]},"filter":null,"granularity":{"type":"all"},"aggregations":[{"type":"count","name":"a0"}],"postAggregations":[],"context":{},"descending":false}], signature=[{d0:STRING, a0:LONG}]) │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
Retrieved 1 row in 0.03s.
```


# 5. 特别注意1

Druid Hadoop Docker安装之前需要关闭SELinux


``` shell
getenforce
```

返回
Enforcing

执行关闭SELinux

``` shell
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```

检查SELinux

``` shell
getenforce
```


# 6. Roll-up聚合操作

Apache Druid可以通过roll-up在数据摄取阶段对原始数据进行汇总。 Roll-up是对选定列集的一级聚合操作，它可以减小存储数据的大小。

本教程中将讨论在一个示例数据集上进行roll-up的结果。

本教程我们假设您已经按照单服务器部署中描述下载了Druid，并运行在本地机器上。

完成加载本地文件和数据查询两部分内容也是非常有帮助的。

## 6.1. 数据准备

使用一个网络流事件数据的小样本，表示在特定时间内从源到目标IP地址的流量的数据包和字节计数。

```json
{"timestamp":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":20,"bytes":9024}
{"timestamp":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":255,"bytes":21133}
{"timestamp":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":11,"bytes":5780}
{"timestamp":"2018-01-01T01:02:14Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":38,"bytes":6289}
{"timestamp":"2018-01-01T01:02:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":377,"bytes":359971}
{"timestamp":"2018-01-01T01:03:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":49,"bytes":10204}
{"timestamp":"2018-01-02T21:33:14Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":38,"bytes":6289}
{"timestamp":"2018-01-02T21:33:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":123,"bytes":93999}
{"timestamp":"2018-01-02T21:35:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8","packets":12,"bytes":2818}
```

示例数据在Druid安装目录`quickstart/tutorial/rollup-data.json`下，我们将使用此接入数据规范来接入数据。

```json
{
  "type" : "index_parallel",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "rollup-tutorial",
      "dimensionsSpec" : {
        "dimensions" : [
          "srcIP",
          "dstIP"
        ]
      },
      "timestampSpec": {
        "column": "timestamp",
        "format": "iso"
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index_parallel",
      "inputSource" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "rollup-data.json"
      },
      "inputFormat" : {
        "type" : "json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}
```

通过在 granularitySpec 选项中设置 rollup : true 来启用Roll-up

注意，我们将srcIP和dstIP定义为维度，将packets和bytes列定义为了longSum类型的指标，并将 queryGranularity 配置定义为 minute。

加载这些数据后，我们将看到如何使用这些定义。

## 6.2. 加载数据

在Druid的根目录下运行以下命令：

```powershell
bin/post-index-task --file quickstart/tutorial/rollup-index.json --url http://localhost:8081
```

脚本运行完成以后，我们将查询数据。

![20201210145621](https://liulv.work/images/img/20201210145621.png)

## 6.3. 查询数据

现在运行 bin/dsql 然后执行查询 select * from "rollup-tutorial"; 来查看已经被摄入的数据

```powershell
[root@localhost apache-druid-0.20.0]# bin/dsql
Welcome to dsql, the command-line client for Druid SQL.
Connected to [http://localhost:8082/].

Type "\h" for help.
dsql> select * from "rollup-tutorial";
┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐
│ __time                   │ bytes  │ count │ dstIP   │ packets │ srcIP   │
├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤
│ 2018-01-01T01:01:00.000Z │  35937 │     3 │ 2.2.2.2 │     286 │ 1.1.1.1 │
│ 2018-01-01T01:02:00.000Z │ 366260 │     2 │ 2.2.2.2 │     415 │ 1.1.1.1 │
│ 2018-01-01T01:03:00.000Z │  10204 │     1 │ 2.2.2.2 │      49 │ 1.1.1.1 │
│ 2018-01-02T21:33:00.000Z │ 100288 │     2 │ 8.8.8.8 │     161 │ 7.7.7.7 │
│ 2018-01-02T21:35:00.000Z │   2818 │     1 │ 8.8.8.8 │      12 │ 7.7.7.7 │
└──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘
Retrieved 5 rows in 0.18s.
```

我们来看发生在 2018-01-01T01:01 的三条原始数据：

```json
{"timestamp":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":20,"bytes":9024}
{"timestamp":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":255,"bytes":21133}
{"timestamp":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2","packets":11,"bytes":5780}
```

这三条数据已经被roll up为以下一行数据：

```powershell
┌──────────────────────────┬────────┬───────┬─────────┬─────────┬─────────┐
│ __time                   │ bytes  │ count │ dstIP   │ packets │ srcIP   │
├──────────────────────────┼────────┼───────┼─────────┼─────────┼─────────┤
│ 2018-01-01T01:01:00.000Z │  35937 │     3 │ 2.2.2.2 │     286 │ 1.1.1.1 │
└──────────────────────────┴────────┴───────┴─────────┴─────────┴─────────┘
```

这输入的数据行已经被按照时间列和维度列 {timestamp, srcIP, dstIP} 在指标列 {packages, bytes} 上做求和聚合.

在进行分组之前，原始输入数据的时间戳按分钟进行标记/布局，这是由于摄取规范中的 "queryGranularity"："minute" 设置造成的。 同样，2018-01-01T01:02 期间发生的这两起事件也已经汇总。


# 7. 配置数据保留规则

假设我们想删除2015年9月12日前12小时的数据，保留2015年9月12日后12小时的数据。

进入到Datasources视图，点击编辑按钮-然后再点击Edit retention rules

![20201210150714](https://liulv.work/images/img/20201210150714.png)

一个规则配置窗口出现了：

![20201210150821](https://liulv.work/images/img/20201210150821.png)

现在点击 + New rule 按钮两次

在上边的规则框中，选择 Load 和 by Interval 然后输入在 by Interval 旁边的输入框中输入 2015-09-12T12:00:00.000Z/2015-09-13T00:00:00.000Z, 副本可以选择保持2，在 _default_tier 中

在下边的规则框中，选择 Drop 和 forever

规则看上去是这样的：

![20201210151143](https://liulv.work/images/img/20201210151143.png)


现在点击 Next, 规则配置过程将要求提供用户名和注释，以便进行更改日志记录。您可以同时输入教程。

现在点击 Save, 可以在Datasources视图中看到新的规则

![20201210151757](https://liulv.work/images/img/20201210151757.png)

给集群几分钟时间应用规则更改，然后转到Druid控制台中的segments视图。2015年9月12日前12小时的段文件现已消失

![20201210151931](https://liulv.work/images/img/20201210151931.png)

生成的保留规则链如下:

loadByInterval 2015-09-12T12/2015-09-13 (12 hours)
dropForever
loadForever (默认规则)
规则链是自上而下计算的，默认规则链始终添加在底部

我们刚刚创建的教程规则链在指定的12小时间隔内加载数据

如果数据不在12小时的间隔内，则规则链下一步将计算 dropForever，这将删除任何数据

dropForever 终止了规则链，有效地覆盖了默认的 loadForever 规则，在这个规则链中永远不会到达该规则

注意，在本教程中，我们定义了一个特定间隔的加载规则

相反，如果希望根据数据的生命周期保留数据（例如，保留从过去3个月到现在3个月的数据），则应定义一个周期性加载规则


# 8. 数据更新

本教程演示如何更新现有数据，同时展示覆盖(Overwrite)和追加(append)的两个方式。

## 8.1. 数据覆盖Overwrite

### 8.1.1. 加载初始数据

本节教程使用的任务摄取规范位于 quickstart/tutorial/updates-init-index.json, 本规范从 quickstart/tutorial/updates-data.json 输入文件创建一个名称为 updates-tutorial 的数据源

提交任务：

```powershell
bin/post-index-task --file quickstart/tutorial/updates-init-index.json --url http://localhost:8081
```

我们有三个包含"动物"维度和"数字"指标的初始行：

```sql
select * from "updates-tutorial";
```

![20201210153051](https://liulv.work/images/img/20201210153051.png)

### 8.1.2. 将旧数据与新数据合并并覆盖

现在我们尝试在 updates-tutorial 数据源追加一些新的数据，我们将从 quickstart/tutorial/updates-data3.json 增加新的数据

quickstart/tutorial/updates-append-index.json 任务规范配置为从现有的 updates-tutorial 数据源和 quickstart/tutorial/updates-data3.json 文件读取数据，该任务将组合来自两个输入源的数据，然后用新的组合数据覆盖原始数据。

提交任务：

```sql
bin/post-index-task --file quickstart/tutorial/updates-append-index.json --url http://localhost:8081
```

查询合并后的数据

```sql
select * from "updates-tutorial";
```

当Druid完成从这个覆盖任务加载新段时，新行将被添加到数据源中。请注意，“Lion”行发生了roll up：

```log
[root@localhost apache-druid-0.20.0]# bin/dsql

Welcome to dsql, the command-line client for Druid SQL.
Connected to [http://localhost:8082/].

Type "\h" for help.
dsql> 
dsql> 
dsql> select * from "updates-tutorial";
┌──────────────────────────┬──────────┬───────┬────────┐
│ __time                   │ animal   │ count │ number │
├──────────────────────────┼──────────┼───────┼────────┤
│ 2018-01-01T01:01:00.000Z │ lion     │     1 │    300 │
│ 2018-01-01T01:01:00.000Z │ tiger    │     1 │    100 │
│ 2018-01-01T03:01:00.000Z │ aardvark │     1 │     42 │
│ 2018-01-01T03:01:00.000Z │ giraffe  │     1 │  14124 │
│ 2018-01-01T05:01:00.000Z │ mongoose │     1 │    737 │
│ 2018-01-01T06:01:00.000Z │ snake    │     1 │   1234 │
│ 2018-01-01T07:01:00.000Z │ octopus  │     1 │    115 │
└──────────────────────────┴──────────┴───────┴────────┘
Retrieved 7 rows in 0.13s.
```

### 8.1.3. 追加数据

现在尝试另一种追加数据的方式

quickstart/tutorial/updates-append-index2.json 任务规范从 quickstart/tutorial/updates-data4.json 文件读取数据，然后追加到 updates-tutorial 数据源。注意到在规范中 appendToExisting 设置为 true

提交任务：

bin/post-index-task --file quickstart/tutorial/updates-append-index2.json --url http://localhost:8081

加载新数据后，我们可以看到"octopus"后面额外的两行。请注意，编号为222的新"bear"行尚未与现有的bear-111行合并，因为新数据保存在单独的段中。

```sql
select * from "updates-tutorial";
```

# 9. 合并段文件

因为每一个段都有一些内存和处理开销，所以有时减少段的总数是有益的，有关详细信息，请查阅段大小优化，所以需要合并段文件。


## 9.1. 加载初始化文件

我们将使用Wikipedia编辑数据，并使用创建每小时段的索引规范

这份规范位于 quickstart/tutorial/ompaction-init-index.json, 它将创建一个名称为 ompaction-tutorial 的数据源

现在加载这份初始数据：

```powershell
bin/post-index-task --file quickstart/tutorial/compaction-init-index.json --url http://localhost:8081
```

> ![WARNING] 请注意，摄取规范中的 maxRowsPerSegment 设置为1000, 这是为了每小时生成多个段，不建议在生产中使用。默认为5000000，可能需要进行调整以优化您的段文件。

提前数据完成后，可以到 http://localhost:8888/unified-console.html#datasources Druid控制台查看新的数据源。

![20201214151548](https://liulv.work/images/img/20201214151548.png)

查看Availability栏，点击对应的数据源的segment查看段

![20201214151916](https://liulv.work/images/img/20201214151916.png)

该数据源有51个段文件，输入数据每小时1-3个段

![20201214152009](https://liulv.work/images/img/20201214152009.png)

对该数据源执行一个 COUNT(*) 查询可以看到39244行数据：

![20201214152151](https://liulv.work/images/img/20201214152151.png)

## 9.2. 合并数据（按小时）

现在我们将合并这51个小的段，在 quickstart/tutorial/compaction-keep-granularity.json 文件中我们包含了一个本教程数据源的合并任务规范。

```json
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "tuningConfig" : {
    "type" : "index_parallel",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000
  }
}
```

该任务会合并 compaction-tutorial 数据源在 2015-09-12/2015-09-13 时间范围内的所有的段

tuningConfig 中的参数控制合并后的段文件集合中有多少个段。

在本教程示例中，每小时只创建一个合并段，因为每小时的行数少于5000000 maxRowsPerSegment（请注意，行总数为39244）。

现在提交这个任务：

```powershell
bin/post-index-task --file quickstart/tutorial/compaction-keep-granularity.json --url http://localhost:8081
```

任务运行结束后，查看datasource,发现只有24个segments

![20201214152843](https://liulv.work/images/img/20201214152843.png)

刷新 Segments 视图, "Segments"视图应显示有24个分段，每小时一个：

![20201214152909](https://liulv.work/images/img/20201214152909.png)

## 9.3. 用新的段粒度合并数据(按天)

合并任务还可以生成不同于输入段粒度的合并段

我们在 quickstart/tutorial/compaction-day-granularity.json 文件中包含了一个可以创建 DAY 粒度的合并任务摄取规范：

```json
{
  "type": "compact",
  "dataSource": "compaction-tutorial",
  "interval": "2015-09-12/2015-09-13",
  "segmentGranularity": "DAY",
  "tuningConfig" : {
    "type" : "index_parallel",
    "maxRowsPerSegment" : 5000000,
    "maxRowsInMemory" : 25000,
    "forceExtendableShardSpecs" : true
  }
}
```

请注意这个合并任务规范中 segmentGranularity 配置项设置为了 DAY, 现在提交这个任务：

```powershell
bin/post-index-task --file quickstart/tutorial/compaction-day-granularity.json --url http://localhost:8081
```

Coordinator将旧的输入段标记为未使用需要一段时间，因此您可能会看到总共有25个段的中间状态。最终，只有一个天粒度的段

![20201214154047](https://liulv.work/images/img/20201214154047.png)

# 10. 数据删除

## 10.1. 加载初始数据

我们将使用Wikipedia编辑数据，并使用创建每小时段的索引规范, 这份规范位于 quickstart/tutorial/deletion-index.json, 它将创建一个名称为 deletion-tutorial 的数据源

现在加载这份初始数据：

```powershell
bin/post-index-task --file quickstart/tutorial/deletion-index.json --url http://localhost:8081
```

当加载完成后，在浏览器中访问http://localhost:8888/unified-console.html#datasources

![20201214154326](https://liulv.work/images/img/20201214154326.png)

## 10.2. 如何永久删除数据

永久删除一个段需要两步：

1. 段必须首先标记为"未使用"。当用户通过Coordinator API手动禁用段时，就会发生这种情况
2. 在段被标记为"未使用"之后，一个Kill任务将从Druid的元数据存储和深层存储中删除任何“未使用”的段

现在让我们通过使用Coordinator API按时间间隔和段id删除一些段。

## 10.3. 通过时间间隔禁用段

让我们在指定的时间间隔内禁用段。这会将间隔中的所有段标记为"未使用"，但不会将它们从深层存储中移除。让我们禁用间隔 2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z中的段，即在18到20小时之间

```powershell
curl -X 'POST' -H 'Content-Type:application/json' -d '{ "interval" : "2015-09-12T18:00:00.000Z/2015-09-12T20:00:00.000Z" }' http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused
```

该命令完成后，您应该看到第18和19小时的段已被禁用：

![20201214154809](https://liulv.work/images/img/20201214154809.png)

请注意，第18小时和第19小时的数据段仍在深层存储中命令:
```powershell
ls -l1 var/druid/segments/deletion-tutorial/
```

![20201214154902](https://liulv.work/images/img/20201214154902.png)

## 10.4. 通过段ID禁用段

让我们按段id禁用一些段。这将再次将段标记为“未使用”，但不会将它们从深层存储中移除。您可以从UI中看到完整的段id，如下所述。

在"Segments"视图中，单击...操作按钮进入View SQL query for table

![20201214155933](https://liulv.work/images/img/20201214155933.png)

查询结果页拉伸segment_id

![1607932812(1)](https://liulv.work/images/img/1607932812(1).png)

![1607932900(1)](https://liulv.work/images/img/1607932900(1).png)

信息框的顶部显示完整的段ID，例如 deletion-tutorial_2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z_2019-02-28T01:11:51.606Z，第14小时的段。

让我们向Coordinator发送一个POST请求来禁用13点和14点的段

```json
{
  "segmentIds":
  [
    "deletion-tutorial_2015-09-12T13:00:00.000Z_2015-09-12T14:00:00.000Z_2019-05-01T17:38:46.961Z",
    "deletion-tutorial_2015-09-12T14:00:00.000Z_2015-09-12T15:00:00.000Z_2019-05-01T17:38:46.961Z"
  ]
}
```

json文件位于quickstart/tutorial/deletion-disable-segments.json, 如下向Coordinator提交一个POST请求：

```sql
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-disable-segments.json http://localhost:8081/druid/coordinator/v1/datasources/deletion-tutorial/markUnused
```

![1607933017(1)](https://liulv.work/images/img/1607933017(1).png)

命令执行完成后，可以看到13时和14时的段已经被禁用：


**但是 但是， 我这里执行禁用不了**

## 10.5. 运行Kill任务

现在我们已经禁用了一些段，我们可以提交一个Kill任务，它将从元数据和深层存储中删除禁用的段。

在 quickstart/tutorial/deletion-kill.json 提供了一个Kill任务的规范

```json
{
  "type": "kill",
  "dataSource": "deletion-tutorial",
  "interval" : "2015-09-12/2015-09-13"
}
```

通过以下的命令将任务提交到Overlord：

```powershell
curl -X 'POST' -H 'Content-Type:application/json' -d @quickstart/tutorial/deletion-kill.json http://localhost:8081/druid/indexer/v1/task
```

任务执行完成后，可以看到已经禁用的段已经被从深度存储中移除了：

![1607933796(1)](https://liulv.work/images/img/1607933796(1).png)


# 11. 数据接入的规范

本章节将指导读者定义摄取规范的过程，指出关键的注意事项和指导原则。

## 11.1. 示例数据

假设我们有如下的网络流数据：

* srcIP: 发送端的IP地址
* srcPort: 发送端的端口
* dstIP: 接收端的IP地址
* dstPort: 接收端的端口
* protocol: IP协议号码
* packets: 传输的数据包数
* bytes: 传输的字节数
* cost: 发送流量的成本

```json
{"ts":"2018-01-01T01:01:35Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":2000, "dstPort":3000, "protocol": 6, "packets":10, "bytes":1000, "cost": 1.4}
{"ts":"2018-01-01T01:01:51Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":2000, "dstPort":3000, "protocol": 6, "packets":20, "bytes":2000, "cost": 3.1}
{"ts":"2018-01-01T01:01:59Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":2000, "dstPort":3000, "protocol": 6, "packets":30, "bytes":3000, "cost": 0.4}
{"ts":"2018-01-01T01:02:14Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":5000, "dstPort":7000, "protocol": 6, "packets":40, "bytes":4000, "cost": 7.9}
{"ts":"2018-01-01T01:02:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":5000, "dstPort":7000, "protocol": 6, "packets":50, "bytes":5000, "cost": 10.2}
{"ts":"2018-01-01T01:03:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":5000, "dstPort":7000, "protocol": 6, "packets":60, "bytes":6000, "cost": 4.3}
{"ts":"2018-01-01T02:33:14Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8", "srcPort":4000, "dstPort":5000, "protocol": 17, "packets":100, "bytes":10000, "cost": 22.4}
{"ts":"2018-01-01T02:33:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8", "srcPort":4000, "dstPort":5000, "protocol": 17, "packets":200, "bytes":20000, "cost": 34.5}
{"ts":"2018-01-01T02:35:45Z","srcIP":"7.7.7.7", "dstIP":"8.8.8.8", "srcPort":4000, "dstPort":5000, "protocol": 17, "packets":300, "bytes":30000, "cost": 46.3}
```

将上面的JSON内容保存到 quickstart/ 中名为insertion-tutorial-data.json 的文件中。

让我们来了解一下定义可加载此数据的摄取规范的过程。

对于本教程，我们将使用本地批索引任务。使用其他任务类型时，摄取规范的某些方面会有所不同，本教程将指出这些方面。

## 11.2. 定义schema

Druid摄取规范中最核心的元素是 dataSchema。 dataSchema 定义了如何将输入数据解析为一组列，这些列将存储在Druid中。

让我们从一个空的 dataSchema 开始，并在完成教程的过程中向它添加字段。

在 quickstart/ 中创建一个名为 insertion-tutorial-index.json 的新文件，其中包含以下内容：


```json
"dataSchema" : {}
```

### 11.2.1. 数据源名称

数据源的名称通过 dataSource 字段来在 dataSchema 中被指定：

"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
}

### 11.2.2. 时间列

dataSchema 需要知道如何从输入数据中提取主时间戳字段。

输入数据中的timestamp列名为"ts"，包含ISO 8601时间戳，因此让我们将包含该信息的 timestampSpec 添加到 dataSchema：

"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "timestampSpec" : {
    "format" : "iso",
    "column" : "ts"
  }
}


## 11.3. Rollup
在接收数据时，我们必须考虑是否要使用rollup

- 如果启用了rollup，我们需要将输入列分为两类，"dimensions"和"metrics"。"dimensions" 是用于rollup的分组列，"metrics"是将要聚合的列。
- 如果禁用了rollup，则所有列都被视为"dimensions"，并且不会发生预聚合。

对于本教程，让我们启用rollup。这是用 dataSchema 上granularitySpec 指定的。

```json
 "granularitySpec" : {
    "rollup" : true
  }
```

综合

```json
'"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "timestampSpec" : {
    "format" : "iso",
    "column" : "ts"
  },
  "granularitySpec" : {
    "rollup" : true
  }
}
```

### 11.3.1. 选择dimension和Metrics

对于这个示例数据集，以下是对"dimensions"和"metrics"的合理划分：

* Dimensions: srcIP, srcPort, dstIP, dstPort, protocol
* Metrics: packets, bytes, cost

这里的维度是一组属性，用于标识IP流量的单向流，而度量则表示由维度分组指定的IP流量的事实。

让我们看看如何在摄取规范中定义这些维度和度量。

```json
 "dimensionsSpec" : {
    "dimensions": [
      "srcIP",
      { "name" : "srcPort", "type" : "long" },
      { "name" : "dstIP", "type" : "string" },
      { "name" : "dstPort", "type" : "long" },
      { "name" : "protocol", "type" : "string" }
    ]
  },
  "metricsSpec" : [
    { "type" : "count", "name" : "count" },
    { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
    { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
    { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
  ],
```


每个维度都有一个 名称 和一个 类型，其中 类型 可以是"long"、"float"、"double"或"string"。

注意，srcIP 是一个"string"维度；对于string维度，只需指定一个维度名称就足够了，因为"string"是默认的维度类型。

还要注意，protocol 是输入数据中的一个数值，但我们将其作为"string"列摄取；Druid将在摄取期间将输入long强制为string。

定义Metrics时，需要指定在rollup期间对该列执行何种类型的聚合

在这里，我们在两个Long类型的Metrics列（数据包和字节）上定义了longSum聚合，并为cost列定义了一个doubleSum聚合

注意，metricsSpec 与 dimensionSpec 或 parseSpec 位于不同的嵌套级别；它与 dataSchema 中的 parser 属于相同的嵌套级别

注意，我们还定义了一个 count 聚合器。计数聚合器将跟踪原始输入数据中有多少行贡献给最终接收数据中的"roll up"行。

综合

```json
"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "timestampSpec" : {
    "format" : "iso",
    "column" : "ts"
  },
    "granularitySpec" : {
    "rollup" : true
  },
  "dimensionsSpec" : {
    "dimensions": [
      "srcIP",
      { "name" : "srcPort", "type" : "long" },
      { "name" : "dstIP", "type" : "string" },
      { "name" : "dstPort", "type" : "long" },
      { "name" : "protocol", "type" : "string" }
    ]
  },
  "metricsSpec" : [
    { "type" : "count", "name" : "count" },
    { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
    { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
    { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
  ]

}
```

## 11.4. No Rollup

如果我们没使用rollup，所有的列都将在 dimensionsSpec 中指定：

```json
   "dimensionsSpec" : {
        "dimensions": [
          "srcIP",
          { "name" : "srcPort", "type" : "long" },
          { "name" : "dstIP", "type" : "string" },
          { "name" : "dstPort", "type" : "long" },
          { "name" : "protocol", "type" : "string" },
          { "name" : "packets", "type" : "long" },
          { "name" : "bytes", "type" : "long" },
          { "name" : "srcPort", "type" : "double" }
        ]
      },
```

综合

```json
"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "timestampSpec" : {
    "format" : "iso",
    "column" : "ts"
  },
    "granularitySpec" : {
    "rollup" : true
  },
  "dimensionsSpec" : {
    "dimensions": [
      "srcIP",
      { "name" : "srcPort", "type" : "long" },
      { "name" : "dstIP", "type" : "string" },
      { "name" : "dstPort", "type" : "long" },
      { "name" : "protocol", "type" : "string" },
      { "name" : "packets", "type" : "long" },
      { "name" : "bytes", "type" : "long" },
      { "name" : "srcPort", "type" : "double" }
    ]
  }
}
```

## 11.5. 定义粒度

在这一点上，我们已经在 dataSchema 中定义了 parser 和metricsSpec，并且我们几乎已经完成了摄取规范的编写

我们需要在 granularitySpec 中设置一些属性：

* granularitySpec类型：uniform 和 arbitrary 是两种支持的类型。在本教程中，我们将使用 uniform 粒度规范，其中所有段都具有统一的间隔（例如：所有段都包含一小时的数据）
* 段粒度：单个段中包含的数据的时间间隔大小？例如：DAY, WEEK
* 时间列中时间戳的bucketing粒度（称为queryGranularity)

### 11.5.1. 段粒度

段粒度由 granularitySpec 中的 SegmentGranularity 属性配置。对于本教程，我们将创建小时段：

```json
"granularitySpec" : {
    "type" : "uniform",
    "segmentGranularity" : "HOUR",
    "rollup" : true
  }
```

我们的输入数据有两个独立小时的事件，因此此任务将生成两个段。

### 11.5.2. 查询粒度

查询粒度由 granularitySpec 中的 queryGranularity 属性配置。对于本教程，我们使用分钟粒度:

```json
"granularitySpec" : {
    "type" : "uniform",
    "segmentGranularity" : "HOUR",
    "queryGranularity" : "MINUTE",
    "rollup" : true
  }
```

要查看查询粒度的影响，让我们从原始输入数据中查看这一行:

```json
{"ts":"2018-01-01T01:03:29Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":5000, "dstPort":7000, "protocol": 6, "packets":60, "bytes":6000, "cost": 4.3}
```

当这一行以分钟查询粒度摄取时，Druid会将该行的时间戳设置为分钟桶：

```json
{"ts":"2018-01-01T01:03:00Z","srcIP":"1.1.1.1", "dstIP":"2.2.2.2", "srcPort":5000, "dstPort":7000, "protocol": 6, "packets":60, "bytes":6000, "cost": 4.3}
```

### 11.5.3. 定义时间间隔（batch only）

对于批处理任务，需要定义时间间隔。时间戳超出时间间隔的输入行将不会被接收。

时间间隔指定在 granularitySpec 配置项中intervals的配置时间范围：

```json
 "granularitySpec" : {
    "type" : "uniform",
    "segmentGranularity" : "HOUR",
    "queryGranularity" : "MINUTE",
    "intervals" : ["2018-01-01/2018-01-02"],
    "rollup" : true
  }
```

综合

```json
"dataSchema" : {
  "dataSource" : "ingestion-tutorial",
  "timestampSpec" : {
    "format" : "iso",
    "column" : "ts"
  },
  "dimensionsSpec" : {
    "dimensions": [
      "srcIP",
      { "name" : "srcPort", "type" : "long" },
      { "name" : "dstIP", "type" : "string" },
      { "name" : "dstPort", "type" : "long" },
      { "name" : "protocol", "type" : "string" }
    ]
  },
  "metricsSpec" : [
    { "type" : "count", "name" : "count" },
    { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
    { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
    { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
  ],
  "granularitySpec" : {
    "type" : "uniform",
    "segmentGranularity" : "HOUR",
    "queryGranularity" : "MINUTE",
    "intervals" : ["2018-01-01/2018-01-02"],
    "rollup" : true
  }
}
```

## 11.6. 定义任务类型

我们现在已经完成了 dataSchema 的定义。剩下的步骤是将我们创建的数据模式放入一个摄取任务规范中，并指定输入源。

dataSchema 在所有任务类型之间共享，但每个任务类型都有自己的规范格式。对于本教程，我们将使用本机批处理摄取任务type类型定义为：

```json
{
  "type" : "index_parallel",
  "spec" : {
    "dataSchema" : {
      ....
    }
  }
}
```

## 11.7. 定义输入源

现在让我们定义输入源，它在 ioConfig 对象中指定。每个任务类型都有自己的 ioConfig 类型。要读取输入数据，我们需要指定一个inputSource。我们前面保存的示例网络流数据需要从本地文件中读取，该文件配置如下：

```json
"ioConfig" : {
  "type" : "index_parallel",
  "inputSource" : {
    "type" : "local",
    "baseDir" : "quickstart/",
    "filter" : "ingestion-tutorial-data.json"
  }
}
```

## 11.8. 定义数据格式

定义数据格式也是在 ioConfig 对象中指定，因为我们的输入数据为JSON字符串，我们使用json的 inputFormat:

```json
 "ioConfig" : {
  "inputFormat" : {
    "type" : "json"
  }
}
```

综合：

```json
{
  "type" : "index_parallel",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "ingestion-tutorial",
      "timestampSpec" : {
        "format" : "iso",
        "column" : "ts"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "srcIP",
          { "name" : "srcPort", "type" : "long" },
          { "name" : "dstIP", "type" : "string" },
          { "name" : "dstPort", "type" : "long" },
          { "name" : "protocol", "type" : "string" }
        ]
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
        { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "HOUR",
        "queryGranularity" : "MINUTE",
        "intervals" : ["2018-01-01/2018-01-02"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index_parallel",
      "inputSource" : {
        "type" : "local",
        "baseDir" : "quickstart/",
        "filter" : "ingestion-tutorial-data.json"
      },
      "inputFormat" : {
        "type" : "json"
      }
    }
  }
}
```

## 11.9. 额外的调整

每一个摄入任务都有一个 tuningConfig 部分，该部分允许用户可以调整不同的摄入参数

例如，我们添加一个 tuningConfig，它为本地批处理摄取任务设置目标段大小：

```json
"tuningConfig" : {
  "type" : "index_parallel",
  "maxRowsPerSegment" : 5000000
}
```

注意：每一类摄入任务都有自己的 tuningConfig 类型

## 11.10. 最终形成的规范

我们已经定义了摄取规范，现在应该如下所示：

```json
{
  "type" : "index_parallel",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "ingestion-tutorial",
      "timestampSpec" : {
        "format" : "iso",
        "column" : "ts"
      },
      "dimensionsSpec" : {
        "dimensions": [
          "srcIP",
          { "name" : "srcPort", "type" : "long" },
          { "name" : "dstIP", "type" : "string" },
          { "name" : "dstPort", "type" : "long" },
          { "name" : "protocol", "type" : "string" }
        ]
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "packets", "fieldName" : "packets" },
        { "type" : "longSum", "name" : "bytes", "fieldName" : "bytes" },
        { "type" : "doubleSum", "name" : "cost", "fieldName" : "cost" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "HOUR",
        "queryGranularity" : "MINUTE",
        "intervals" : ["2018-01-01/2018-01-02"],
        "rollup" : true
      }
    },
    "ioConfig" : {
      "type" : "index_parallel",
      "inputSource" : {
        "type" : "local",
        "baseDir" : "quickstart/",
        "filter" : "ingestion-tutorial-data.json"
      },
      "inputFormat" : {
        "type" : "json"
      }
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "maxRowsPerSegment" : 5000000
    }
  }
}
```

## 11.11. 提交任务和查询数据

在Druid根目录下执行下面命令：

```powershell
bin/post-index-task --file quickstart/ingestion-tutorial-index.json --url http://localhost:8081
```

脚本运行完成后我们可以查询数据。

运行 bin/dsql 中运行 select * from "ingestion-tutorial" 查询我们摄入什么样的数据

![20201215115911](https://liulv.work/images/img/20201215115911.png)

# 12. 输入数据变换

本章节将介绍如何使用转换规范在接收期间过滤和转换输入数据。

## 12.1. 样例数据

我们在 quickstart/tutorial/transform-data.json 中包括了样例数据，为了方便我们展示一下：

```json
{"timestamp":"2018-01-01T07:01:35Z","animal":"octopus",  "location":1, "number":100}
{"timestamp":"2018-01-01T05:01:35Z","animal":"mongoose", "location":2,"number":200}
{"timestamp":"2018-01-01T06:01:35Z","animal":"snake", "location":3, "number":300}
{"timestamp":"2018-01-01T01:01:35Z","animal":"lion", "location":4, "number":300}
```

## 12.2. 使用转换规范加载数据

在转换规范中，我们有两个表达式转换：

* super-animal: 在 "animal" 列的值前加上"super-"。这将用转换后的版本覆盖 animal 列，因为转换的名称是 animal
* triple-number: 将数字列乘以3, 这将创建一个新的三位数列。注意，我们同时接收原始列和转换列

另外，我们有一个包含三个子句的OR过滤器：

* super-animal 值匹配"super-mongoose"
* triple-number 值匹配300
* location值匹配3
  
这个过滤器选择前3行，它将排除输入数据中的最后一个"lion"行。请注意，过滤器是在转换之后应用的。这个在spec - dataSchema -transformSpec 节点下配置，如下：


```json
  "transformSpec": {
        "transforms": [
          {
            "type": "expression",
            "name": "animal",
            "expression": "concat('super-', animal)"
          },
          {
            "type": "expression",
            "name": "triple-number",
            "expression": "number * 3"
          }
        ],
        "filter": {
          "type":"or",
          "fields": [
            { "type": "selector", "dimension": "animal", "value": "super-mongoose" },
            { "type": "selector", "dimension": "triple-number", "value": "300" },
            { "type": "selector", "dimension": "location", "value": "3" }
          ]
        }
      }
```

完整

```json
{
  "type" : "index_parallel",
  "spec" : {
    "dataSchema" : {
      "dataSource" : "transform-tutorial",
      "timestampSpec": {
        "column": "timestamp",
        "format": "iso"
      },
      "dimensionsSpec" : {
        "dimensions" : [
          "animal",
          { "name": "location", "type": "long" }
        ]
      },
      "metricsSpec" : [
        { "type" : "count", "name" : "count" },
        { "type" : "longSum", "name" : "number", "fieldName" : "number" },
        { "type" : "longSum", "name" : "triple-number", "fieldName" : "triple-number" }
      ],
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "week",
        "queryGranularity" : "minute",
        "intervals" : ["2018-01-01/2018-01-03"],
        "rollup" : true
      },
      "transformSpec": {
        "transforms": [
          {
            "type": "expression",
            "name": "animal",
            "expression": "concat('super-', animal)"
          },
          {
            "type": "expression",
            "name": "triple-number",
            "expression": "number * 3"
          }
        ],
        "filter": {
          "type":"or",
          "fields": [
            { "type": "selector", "dimension": "animal", "value": "super-mongoose" },
            { "type": "selector", "dimension": "triple-number", "value": "300" },
            { "type": "selector", "dimension": "location", "value": "3" }
          ]
        }
      }
    },
    "ioConfig" : {
      "type" : "index_parallel",
      "inputSource" : {
        "type" : "local",
        "baseDir" : "quickstart/tutorial",
        "filter" : "transform-data.json"
      },
      "inputFormat" : {
        "type" :"json"
      },
      "appendToExisting" : false
    },
    "tuningConfig" : {
      "type" : "index_parallel",
      "maxRowsPerSegment" : 5000000,
      "maxRowsInMemory" : 25000
    }
  }
}
```

## 12.3. 提交任务

现在提交位于 quickstart/tutorial/transform-index.json 的任务：

```powershell
bin/post-index-task --file quickstart/tutorial/transform-index.json --url http://localhost:8081
```

## 12.4. 查询已转换的数据

运行 bin/dsql 提交 select * from "transform-tutorial" 查询来看摄入的数据：

![20201215161618](https://liulv.work/images/img/20201215161618.png)


# 13. 架构设计

Druid有一个多进程、分布式的架构，该架构设计为云友好且易于操作。每个Druid进程都可以独立配置和扩展，在集群上提供最大的灵活性。这种设计还提供了增强的容错能力：一个组件的中断不会立即影响其他组件。

## 13.1. 进程与服务

Druid有若干不同类型的进程，简单描述如下：

* [Coordinator](#) 进程管理集群中数据的可用性
* [Overlord](#) 进程控制数据摄取负载的分配
* [Broker](#) 进程处理来自外部客户端的查询请求
* [Router](#) 进程是一个可选进程，可以将请求路由到Brokers、Coordinators和Overlords
* [Historical](#) 进程存储可查询的数据
* [MiddleManager](#) 进程负责摄取数据

Druid进程可以按照您喜欢的方式部署，但是为了便于部署，我们建议将它们组织成三种服务器类型：Master、Query和Data。

* **Master**: 运行Coordinator和Overlord进程，管理数据可用性和摄取
* **Query:** 运行Broker和可选的Router进程，处理来自外部客户端的请求
* **Data:** 运行Historical和MiddleManager进程，执行摄取负载和存储所有可查询的数据

关于进程和服务组织的更多信息，可以查看Druid[进程与服务](#)

## 13.2. 外部依赖

除了内置的进程类型外，Druid同时有三个外部依赖，它们旨在利用现有的基础设施（如果有的话）。

## 13.3. 深度存储

每个Druid服务器都可以访问的共享文件存储。在集群部署中，通常使用一个像S3或HDFS这样的分布式对象存储，或者是一个网络挂载的文件系统。在单服务器部署中，通常使用本地磁盘。Druid使用深度存储来存储任何已被系统接收的数据。

Druid只使用深度存储作为数据备份，并作为在后台Druid进程之间传输数据的一种方式。为了响应查询，Historical进程不会从深层存储中读取数据，而是从本地磁盘读取在执行任何查询之前预缓存的段，这意味着Druid在查询期间不需要访问深层存储，这有助于它提供尽可能最好的查询延迟。这也意味着您必须在深层存储和所有Historical进程中都有足够的磁盘空间来存储计划加载的数据。

深度存储是Druid弹性、容错设计的重要组成部分。即使每个数据服务器都丢失并重新配置，Druid也可以从深层存储启动。

有关更多详细信息，请参见深度存储

## 13.4. 元数据存储

元数据存储包含各种共享的系统元数据，如段可用性信息和任务信息。在集群部署中，通常使用像PostgreSQL或MySQL这样的传统RDBMS。在单服务器部署中，通常使用本地存储的Apache Derby数据库。

有关更多详细信息，请参见元数据存储.

## 13.5. Zookeeper

用于内部服务发现、协调和领导选举。

有关更多详细信息，请参见Zookeeper

## 13.6. 架构图

下图显示了使用建议的Master/Query/Data服务组织，查询和数据如何通过此体系结构流动：

![20201215180836](https://liulv.work/images/img/20201215180836.png)

## 13.7. 存储设计

**数据源和段**

Druid数据被存储在"datasources"中，类似于传统RDBMS中的表。每一个数据源可以根据时间进行分区，可选地还可以进一步根据其他属性进行分区。每一个时间范围称为一个"块（chunk）"(例如，如果数据源按天分区，则为一天)。在一个块中，数据被分为一个或者多个"段（segments）"。每个段是一个单独的文件，一般情况下由数百万条数据组成。由于段被组织成时间块，因此有时将段视为存在于如下时间线上是有帮助的：

![20201215181028](https://liulv.work/images/img/20201215181028.png)

一个数据源可能有几个段到几十万甚至几百万个段。每个段都是在MiddleManager上创建的，但此时段是可变的和未提交的。段构建过程包括以下步骤，旨在生成一个紧凑且支持快速查询的数据文件：

* 转换为列格式
* 构建位图索引
* 使用不同的算法进行压缩
  * 字符串列id存储最小化的字典编码
  * 对位图索引做压缩
  * 所有列的类型感知压缩













