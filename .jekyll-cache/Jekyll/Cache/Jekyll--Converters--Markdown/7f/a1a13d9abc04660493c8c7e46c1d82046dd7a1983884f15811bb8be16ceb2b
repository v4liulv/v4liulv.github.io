I"a<h1 id="版本信息">版本信息</h1>

<table>
  <thead>
    <tr>
      <th>组件</th>
      <th>版本</th>
      <th>Tbds</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Hadoop</td>
      <td>2.7.3</td>
      <td> </td>
    </tr>
    <tr>
      <td>Spark</td>
      <td>2.3.2</td>
      <td> </td>
    </tr>
    <tr>
      <td>Kafka</td>
      <td>1.0.0</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h1 id="本地通过yarn提交sparkstructuredstreaming">本地通过Yarn提交SparkStructuredStreaming</h1>

<div class="language-powershell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="nf">groupId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">org.apache.spark</span><span class="w">
</span><span class="nx">artifactId</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">spark-sql-kafka-0-10_2.11</span><span class="w">
</span><span class="nx">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">2.3.2</span><span class="w">
</span></pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="springboot整合sparkonyarn远程提交运行spark-bug">SpringBoot整合SparkOnYarn远程提交运行Spark-BUG</h1>

<h2 id="no-filesystem-for-scheme-hdfs">No FileSystem for scheme: hdfs</h2>

<p>Spark运行程序报错No FileSystem for scheme: hdfs</p>

<p>缺少hadoop-hdfs包，在hadoop-common和hadoop-hdfs包的META-INF\services下均有一个org.apache.hadoop.fs.FileSystem文件，但文件内容是不同的。</p>

<p>解决办法：Spark程序spark.yarn.jars包目录引入hadoop-hdfs包。</p>

<h2 id="noclassdeffounderror-comfasterxmljacksondatabindobjectmapper">NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapper</h2>

<p>SpringBoot启动报错</p>

<p>原因：依赖冲突，SparkSql和SpringBoot包冲突</p>

<p>解决办法：</p>
<ul>
  <li>
    <p>办法：SparkSQL只是运行在Yarn上配置spark.yarn.jars包即可，程序不需要Spark包，Maven把Spring相关依赖设置为<code class="language-plaintext highlighter-rouge">&lt;scope&gt;provided&lt;/scope&gt;</code></p>
  </li>
  <li>
    <p>办法：使用jackson-databind-2.10.3依赖版本</p>
  </li>
</ul>

<h2 id="gsonbuilder报错">GsonBuilder报错</h2>

<p>application so that it contains a single, compatible version of com.google.gson.GsonBuilder</p>

<p>解决办法: 添加依赖坐标</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>com.google.code.gson<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>gson<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>2.6<span class="nt">&lt;/version&gt;</span>
<span class="nt">&lt;/dependency&gt;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="user-did-not-initialize-spark-context">User did not initialize spark context!</h2>

<p>Uncaught exception: java.lang.IllegalStateException: User did not initialize spark context!</p>

<p>Spark运行程序报错，配置了spark.submit.deployMode为cluster，还配置了spark.driver.host为客户端ip</p>

<p>解决办法：
去掉spark.driver.host配置，这个配置只有在spark.submit.deployMode为client</p>

<h2 id="调度异常堵塞">调度异常堵塞</h2>

<p>[AccumulationJob-1] accumulation schedule delay, expect:1609618320000, actual: 1609618448754</p>

<p>yarn application -list</p>

<p>yarn application -kill</p>

:ET